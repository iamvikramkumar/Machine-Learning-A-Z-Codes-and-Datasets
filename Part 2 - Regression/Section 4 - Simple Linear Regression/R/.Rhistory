T1=norm_IV[1:20,]
T2=norm_IV[21:32,]
trainlabels=DV[1:20,]
testlabels=DV[21:32,]
library(class)
pred_labels=knn(T1,T2,cl=trainlabels,k=7)
library(caret)
confusionMatrix(testlabels,pred_labels)
library(ggplot2)
d1=diamonds
View(d1)
library(ggplot2)
str(diamonds)
table(diamonds$cut)
iv=diamonds[5:10]
dv=diamonds[2]
dv$cut=factor(dv$cut,levels=c("Fair","Good","Very Good","Premium","Ideal"),
labels=c(1,2,3,4,5))
normalise=function(x)
{
return ((x-min(x))/(max(x)-min(x)))
}
iv=as.data.frame(lapply(iv,normalise))
#split train and test set from iv
train=iv[1:40000,]
test=iv[40001:53940,]
#split labels from dv
trainlabels=dv[1:40000,1]
testlabels=dv[40001:53940,1]
#apply knn
library(class)
test_pred=knn(train,test,trainlabels,k=89)
class(train)
class(trainlabels)
View(trainlabels)
trainlabels=as.factor(trainlabels)
View(trainlabels)
trainlabels
trainlabels=as.data.frame(trainlabels)
View(trainlabels)
library(ggplot2)
View(diamonds)
str(diamonds)
table(diamonds$cut)
iv=diamonds[5:10]
View(iv)
dv=diamonds[2]
View(dv)
dv$cut=factor(dv$cut,levels=c("Fair","Good","Very Good","Premium","Ideal"),
labels=c(1,2,3,4,5))
normalise=function(x)
{
return ((x-min(x))/(max(x)-min(x)))
}
iv=as.data.frame(lapply(iv,normalise))
#split train and test set from iv
train=iv[1:40000,]
test=iv[40001:53940,]
class(train)
#split labels from dv
trainlabels=dv[1:40000,1]
testlabels=dv[40001:53940,1]
class(trainlabels)
trainlabels=factor(trainlabels,levels=c(1,2,3,4,5))
class(trainlabels)
#apply knn
library(class)
test_pred=knn(train,test,trainlabels,k=89)
sum(is.na(trainlabels))
miss=trainlabels[!complete.cases(trainlabels),]
is.na(trainlabels)
View(is.na(trainlabels))
View(trainlabels)
summary(trainlabels)
View(trainlabels)
library(ggplot2)
View(diamonds)
str(diamonds)
table(diamonds$cut)
iv=diamonds[5:10]
View(iv)
dv=diamonds[2]
View(dv)
dv$cut=factor(dv$cut,levels=c("Fair","Good","Very Good","Premium","Ideal"),
labels=c(1,2,3,4,5))
View(dv)
normalise=function(x)
{
return ((x-min(x))/(max(x)-min(x)))
}
iv=as.data.frame(lapply(iv,normalise))
#split train and test set from iv
train=iv[1:40000,]
test=iv[40001:53940,]
class(train)
#split labels from dv
trainlabels=dv[1:40000,1]
View(trainlabels)
summary(trainlabels)
testlabels=dv[40001:53940,1]
View(testlabels)
summary(testlabels)
#apply knn
library(class)
test_pred=knn(train,test,trainlabels,k=89)
View(train)
sum(is.na(train))
View(test)
sum(is.na(test))
#split labels from dv
trainlabels=dv[1:40000,1]
View(trainlabels)
summary(trainlabels)
testlabels=dv[40001:53940,1]
View(testlabels)
View(diamonds)
d1=diamonds[1,5,6,7,8,9,10]
View(d1)
d1=as.data.frame(diamonds[1,5,6,7,8,9,10])
View(d1)
d1=diamonds[,c(1,5,6,7,8,9,10)]
View(d1)
library(ggplot2)
View(diamonds)
str(diamonds)
table(diamonds$cut)
d1=diamonds[,c(2,5,6,7,8,9,10)]
View(d1)
d1$cut=factor(d1$cut,levels=c("Fair","Good","Very Good","Premium","Ideal"),
labels=c(1,2,3,4,5))
View(d1)
normalise=function(x)
{
return ((x-min(x))/(max(x)-min(x)))
}
norml=as.data.frame(lapply(d1[2:7],normalise))
summary(norml)
#split train and test set from iv
train=d1[1:40000,]
View(train)
sum(is.na(train))
test=d1[40001:53940,]
View(test)
sum(is.na(test))
#split train and test set from iv
train=norml[1:40000,]
View(train)
sum(is.na(train))
test=d1[40001:53940,]
View(test)
test=norml[40001:53940,]
View(test)
sum(is.na(test))
#split labels from dv
trainlabels=d1[1:40000,1]
View(trainlabels)
summary(trainlabels)
testlabels=d1[40001:53940,1]
View(testlabels)
summary(testlabels)
#apply knn
library(class)
test_pred=knn(train,test,trainlabels,k=89)
class(train)
class(trainlabels)
class(d1$cut)
#split labels from dv
trainlabels=d1[1:40000,d1$cut]
View(trainlabels)
class(trainlabels)
#split labels from dv
trainlabels=d1$cut[1:40000]
summary(trainlabels)
class(trainlabels)
class(train)
class(trainlabels)
#apply knn
library(class)
test_pred=knn(train,test,trainlabels,k=89)
confusionMatrix(test_pred,testlabels)
library(caret)
confusionMatrix(test_pred,testlabels)
class(test_pred)
class(testlabels)
testlabels=d1$cut[40001:53940]
class(train)
class(testlabels)
#apply knn
library(class)
test_pred=knn(train,test,trainlabels,k=89)
class(test_pred)
library(caret)
confusionMatrix(test_pred,testlabels)
d1=read.csv(file.choose())
View(d1)
table(d1$Outcome)
sum(is.na(d1))
summary(d1)
normalise=function(x)
{
return ((x-min(x))/(max(x)-min(x)))
}
norm_d1=as.data.frame(lapply(d1[1:8]),normalise)
norm_d1=as.data.frame(lapply(d1[1:8],normalise))
View(norm_d1)
summary(norm_d1)
library(caTools)
set.seed(123)
split = sample.split(d1$Outcome, SplitRatio = 0.7)
training_set = subset(d1, split == TRUE)
test_set = subset(d1, split == FALSE)
View(training_set)
View(test_set)
split = sample.split(norm_d1$Outcome, SplitRatio = 0.7)
training_set = subset(norm_d1, split == TRUE)
test_set = subset(norm_d1, split == FALSE)
View(training_set)
View(training_set)
View(test_set)
mtcars
View(mtcars)
d1=mtcars
View(d1)
str(d1)
table(d1$cyl)
table(d1$vs)
table(d1$am)
table(d1$gear)
table(d1$carb)
table(d1$mpg)
View(d1)
iv=d1[c(1,3,4,5,6,7)]
View(iv)
dv=d1["vs"]
View(dv)
class(dv)
dv$vs=factor(dv$vs,levels=c(0,1))
class(dv)
dv$vs=factor(dv$vs,levels=c(0,1))
class(dv$vs)
summary(iv)
return ((x-min(x))/(max(x)-min(x)))
normalise=function(x)
{
return ((x-min(x))/(max(x)-min(x)))
}
norm_iv=as.data.frame(lapply(iv,normalise))
summary(norm_iv)
split = sample.split(norm_iv$mpg, SplitRatio = 0.7)
library(caTools)
set.seed(123)
split = sample.split(norm_iv$mpg, SplitRatio = 0.7)
training_set = subset(norm_iv, split == TRUE)
test_set = subset(norm_iv, split == FALSE)
View(training_set)
View(test_set)
set.seed(123)
split = sample.split(dv$vs, SplitRatio = 0.7)
trainlabels = subset(dv, split == TRUE)
testlabels = subset(dv, split == FALSE)
d1=mtcars
str(d1)
table(d1$cyl)
table(d1$vs)
table(d1$am)
table(d1$gear)
table(d1$carb)
table(d1$mpg)
iv=d1[c(1,3,4,5,6,7)]
dv=d1["vs"]
class(dv)
dv$vs=factor(dv$vs,levels=c(0,1))
class(dv$vs)
summary(iv)
normalise=function(x)
{
return ((x-min(x))/(max(x)-min(x)))
}
norm_iv=as.data.frame(lapply(iv,normalise))
summary(norm_iv)
trained=norm_iv[1:22,]
testset=norm_iv[23:32,]
trainlabels=dv[1:22,1]
testlabels=dv[23:32,1]
library(class)
pred=knn(trained,testset,trainlabels,k=8)
class(trainlabels)
trainlabels=dv$vs[1:22,1]
trainlabels=dv$vs[1:22]
testlabels=dv$vs[23:32]
library(class)
pred=knn(trained,testset,trainlabels,k=8)
library(caret)
confusionMatrix(pred,testlabels)
library(ggplot2)
d1=diamonds
View(d1)
#load the dataset Social_Network_Ads.csv
d1=read.csv(choose.files())
View(d1)
Str(d1)
str(d1)
summary(d1)
class(d1$Purchased)
table(d1$Purchased)
d1=d1[3:5]
d1$Purchased=factor(d1$Purchased,levels=c(0,1))
class(d1$Purchased)
d1[-3]=scale(d1[-3])
summary(d1)
library(caTools)
set.seed(90)
split=sample.split(d1$Purchased,,SplitRatio = 0.75)
split
train=subset(d1,split==TRUE)
test=subset(d1,split==FALSE)
install.packages('e1071')
#install.packages('e1071')
library(e1071)
classifier=naiveBayes(x=train[-3],y=train[3])
predicted=predict(classifier,test[-3])
View(predicted)
View(as.data.frame(predicted))
library(caret)
confusionMatrix(test[3],predicted)
class(test[3])
d2=data.frame("Age"=37,"EstimatedSalary"=150000)
pred=predict(classifier,d2)
pred
cm=table(test[3],predicted)
cm=table(test[,3],predicted)
cm
confusionMatrix(cm)
table(test[,3],predicted)
confusionMatrix(test[,3],predicted)
#Load the dataset "Social_Network_Ads.csv"
d1=read.csv(file.choose())
View(d1)
str(d1)
a=10
a
a=20
a
View(d1)
d1=d1[3:5]
View(d1)
table(d1$Age)
table(d1$EstimatedSalary)
table(d1$Purchased)
class(d1$Purchased)
d1$Purchased=factor(d1$Purchased,levels=c(0,1))
class(d1$Purchased)
d1[-3]=scale(d1[-3])
summary(d1)
library(caTools)
library(caTools)
split=sample.split(d1$Purchased,SplitRatio = 0.8)
split
View(train)
train=subset(d1,split==TRUE)
test=subset(d1,split==FALSE)
View(train)
source("C:/Users/Veerpal Kaur/Desktop/NaiveBayes.R", echo=TRUE)
View(test)
library(e1071)
classifier=naiveBayes(x=d1[-3],y=d1[,3])
pred=predict(classifier,test[-3])
library(caret)
confusionMatrix(test[,-3],pred)
confusionMatrix(test[,3],pred)
classifier=naiveBayes(x=train[-3],y=train[,3])
pred=predict(classifier,test[-3])
library(caret)
confusionMatrix(test[,3],pred)
classifier
d2=data.frame("Age"=38,"EstimatedSalary"=89000)
d2
p1=predict(classifer,d2)
p1=predict(classifier,d2)
p1
classifier
d3=read_delim(file.choose(),delim = "\t",col_names = FALSE)
library(readr)
d3=read_delim(file.choose(),delim = "\t",col_names = FALSE)
d3=read_delim(file.choose(),delim = "\t")
v1 = 10:12
v2 = c("a", "b","c")
df = data.frame(v1,v2)
str(df)
rno=c(1:5)
name=c("A","B","C","D","E")
age = c(15,18,14,20,19)
df=data.frame(rno,name,age)
df
df[1,2]
a)	patient = c(1,2,3,4,5)
diagnosis = c("B","B","M","B", "M")
df=data.frame(patient,diagnosis)
diagnosis=factor(diagnosis,labels= c("Benign", "Malignant"))
patient = c(1,2,3,4,5)
diagnosis = c("B","B","M","B", "M")
df=data.frame(patient,diagnosis)
df
diagnosis=factor(diagnosis,labels= c("Benign", "Malignant"))
df
df$diagnosis=factor(df$diagnosis,labels=c("Benign", "Malignant"))
df
library(bibliometrix)
biblioshiny()
library(bibliometrix)
biblioshiny()
library(bibliometrix)
biblioshiny()
library(bibliometrix)
setwd("C:/1_Study material/Machine Learning using R/Machine Learning A-Z (Codes and Datasets)/Part 2 - Regression/Section 4 - Simple Linear Regression/R")
#load sales dataset
d2=read.csv("Sales.csv")
d2=d2[-1]
colnames(d2)=c("Sales","DollarsSpent")
sum(is.na(d2))
#splitting dataset
library(caTools)
split=sample.split(d2$Sales,SplitRatio = 0.8)
training=subset(d2,split==TRUE)
testing=subset(d2,split==FALSE)
#Linear Model building
regressor=lm(formula=Sales~.,data=training)
y3=predict(regressor,testing[-1])
library(ModelMetrics)
rmse(y3,testing$Sales)
cor(Sales,DollarsSpent)
cor(d2$Sales,d2$DollarsSpent)
cor(d2$DollarsSpent,d2$Sales)
d3=read.csv("Salary_Data.csv")
library(readxl)
d=read_excel("Height_Weight.xls")
View(d)
data = read_excel("Height_Weight.xls")
data = data[-1]
colnames(data)
colnames(data) = c("Height", "Weight")
colnames(data)
library(caTools)
split = sample.split(data$Height, SplitRatio = 0.75)
train = subset(data , split==TRUE)
test = subset(data, split==FALSE)
regressor = lm(formula = Height~.,data = train)
y = predict(regressor, test[-1])
library(ModelMetrics)
rmse(y, test$Height)
cor(data$Height,data$Weight)
d3=read.csv(file.choose())
View(d3)
View(d3)
View(d3)
d3=d3[c(6,7)]
colnames(d3)=c("Temp","Salinity")
sum(is.na(d3))
d3=read.csv(file.choose())
d3=read.csv(file.choose())
View(d3)
d3=d3[c(6,7)]
colnames(d3)=c("Temp","Salinity")
sum(is.na(d3))
d3$Temp=ifelse(is.na(d3$Temp),mean(d3$Temp,na.rm = TRUE),d3$Temp)
sum(is.na(d3))
d3$Salinity=ifelse(is.na(d3$Salinity),
mean(d3$Salinity,na.rm = TRUE),
d3$Salinity)
sum(is.na(d3))
d3=read.csv(file.choose())
View(d3)
str(d3)
lapply(d3,summary)
d3=d3[c(1,5,6,7,9,14,44,47,50)]
lapply(d3,summary)
cor(d3$Cst_Cnt,d3$Depthm)
sum(is.na(d3$Cst_Cnt))
sum(is.na(d3$Depthm))
cor(d3$T_degC,d3$Salnty)
sum(is.na(d3$T_degC))
d3$T_degC=ifelse(is.na(d3$T_degC),
mean(d3$T_degC,na.rm=T),
d3$T_degC)
sum(is.na(d3$T_degC))
cor(d3$T_degC,d3$Salnty)
sum(is.na(d3$Salnty))
d3$Salnty=ifelse(is.na(d3$Salnty),mean(d3$T_degC,na.rm=T),
d3$T_degC)
d3$Salnty=ifelse(is.na(d3$Salnty),mean(d3$Salnty,na.rm=T),
d3$Salnty)
sum(is.na(d3$Salnty))
cor(d3$T_degC,d3$Salnty)
cor(d3$T_degC,d3$Salnty)
split=sample.split(d3$Salnty,SplitRatio=0.75)
train=subset(d3,split==TRUE)
test=subset(d3,split==FALSE)
#model implementation
regressor=lm(formula=Salnty~T_degC,data=train)
summary(regressor)
ypred=predict(regressor,test$T_degC)
ypred=predict(regressor,test[3])
rmse(ypred,test[4])
rmse(ypred,test$Salnty)
#model implementation
regressor=lm(formula=Salnty~T_degC+STheta+DarkAq,data=train)
summary(regressor)
ypred=predict(regressor,test[3])
colnames(d3)
ypred=predict(regressor,test[c(3,5,7)])
library(ModelMetrics)
rmse(ypred,test$Salnty)
