IV=d1[c(1,3,4,5,6,7)]
View(IV)
normalise=function(x)
{
return ((x-min(x))/(max(x)-min(x)))
}
norm_IV=as.data.frame(lapply(IV,normalise))
View(norm_IV)
T1=norm_IV[sample(norm_IV,20)]
T1=norm_IV[sample(norm_IV,20),replace=TRUE]
T1=norm_IV[sample(norm_IV,20),replace=FALSE]
T1=norm_IV[sample(norm_IV,20,replace=FALSE)]
T1=norm_IV[sample(norm_IV,20,replace=TRUE)]
class(norm_IV)
sample(norm_IV,20,replace=TRUE)
T1=norm_IV[sample(norm_IV,20,replace=TRUE),]
T1=norm_IV[c(sample(norm_IV,20,replace=TRUE)),]
T1=norm_IV[c(sample(nrow(norm_IV),20,replace=TRUE)),]
View(T1)
T2=norm_IV[c(sample(nrow(norm_IV),12,replace=TRUE)),]
View(T2)
trainlabels=DV[1]
DV=d1[2]
trainlabels=DV[1]
testlabels=DV[1]
trainlabels=DV[c(sample(nrow(norm_IV),20,replace=TRUE)),]
testlabels=DV[cc(sample(nrow(norm_IV),12,replace=TRUE)),]
pred_labels=knn(T1,T2,cl=trainlabels,k=7)
library(class)
pred_labels=knn(T1,T2,cl=trainlabels,k=7)
library(caret)
confusionMatrix(testlabels,pred_labels)
library(caret)
confusionMatrix(testlabels,pred_labels)
testlabels=DV[c(sample(nrow(norm_IV),12,replace=TRUE)),]
library(class)
pred_labels=knn(T1,T2,cl=trainlabels,k=7)
library(caret)
confusionMatrix(testlabels,pred_labels)
DV$cyl=factor(DV$cyl,levels=c(4,6,8),labels=c(1,2,3))
normalise=function(x)
{
return ((x-min(x))/(max(x)-min(x)))
}
norm_IV=as.data.frame(lapply(IV,normalise))
class(norm_IV)
T1=norm_IV[c(sample(nrow(norm_IV),20,replace=TRUE)),]
T2=norm_IV[c(sample(nrow(norm_IV),12,replace=TRUE)),]
trainlabels=DV[c(sample(nrow(norm_IV),20,replace=TRUE)),]
testlabels=DV[c(sample(nrow(norm_IV),12,replace=TRUE)),]
library(class)
pred_labels=knn(T1,T2,cl=trainlabels,k=7)
library(caret)
confusionMatrix(testlabels,pred_labels)
View(mtcars)
d1=mtcars
IV=d1[c(1,3,4,5,6,7)]
DV=d1[2]
normalise=function(x)
{
return ((x-min(x))/(max(x)-min(x)))
}
norm_IV=as.data.frame(lapply(IV,normalise))
T1=norm_IV[c(sample(nrow(norm_IV),20,replace=TRUE)),]
T2=norm_IV[c(sample(nrow(norm_IV),12,replace=TRUE)),]
trainlabels=DV[c(sample(nrow(norm_IV),20,replace=TRUE)),]
testlabels=DV[c(sample(nrow(norm_IV),12,replace=TRUE)),]
class(DV$cyl)
library(class)
pred_labels=knn(T1,T2,cl=trainlabels,k=7)
library(caret)
confusionMatrix(testlabels,pred_labels)
DV$cyl=factor(DV$cyl,levels=c(4,6,8),labels=c(1,2,3))
normalise=function(x)
{
return ((x-min(x))/(max(x)-min(x)))
}
norm_IV=as.data.frame(lapply(IV,normalise))
class(norm_IV)
T1=norm_IV[c(sample(nrow(norm_IV),20,replace=TRUE)),]
T2=norm_IV[c(sample(nrow(norm_IV),12,replace=TRUE)),]
trainlabels=DV[c(sample(nrow(norm_IV),20,replace=TRUE)),]
testlabels=DV[c(sample(nrow(norm_IV),12,replace=TRUE)),]
class(DV$cyl)
library(class)
pred_labels=knn(T1,T2,cl=trainlabels,k=7)
library(caret)
confusionMatrix(testlabels,pred_labels)
T1=norm_IV[1:20,]
T2=norm_IV[21:32,]
trainlabels=DV[1:20,]
testlabels=DV[21:32,]
library(class)
pred_labels=knn(T1,T2,cl=trainlabels,k=7)
library(caret)
confusionMatrix(testlabels,pred_labels)
library(ggplot2)
d1=diamonds
View(d1)
library(ggplot2)
str(diamonds)
table(diamonds$cut)
iv=diamonds[5:10]
dv=diamonds[2]
dv$cut=factor(dv$cut,levels=c("Fair","Good","Very Good","Premium","Ideal"),
labels=c(1,2,3,4,5))
normalise=function(x)
{
return ((x-min(x))/(max(x)-min(x)))
}
iv=as.data.frame(lapply(iv,normalise))
#split train and test set from iv
train=iv[1:40000,]
test=iv[40001:53940,]
#split labels from dv
trainlabels=dv[1:40000,1]
testlabels=dv[40001:53940,1]
#apply knn
library(class)
test_pred=knn(train,test,trainlabels,k=89)
class(train)
class(trainlabels)
View(trainlabels)
trainlabels=as.factor(trainlabels)
View(trainlabels)
trainlabels
trainlabels=as.data.frame(trainlabels)
View(trainlabels)
library(ggplot2)
View(diamonds)
str(diamonds)
table(diamonds$cut)
iv=diamonds[5:10]
View(iv)
dv=diamonds[2]
View(dv)
dv$cut=factor(dv$cut,levels=c("Fair","Good","Very Good","Premium","Ideal"),
labels=c(1,2,3,4,5))
normalise=function(x)
{
return ((x-min(x))/(max(x)-min(x)))
}
iv=as.data.frame(lapply(iv,normalise))
#split train and test set from iv
train=iv[1:40000,]
test=iv[40001:53940,]
class(train)
#split labels from dv
trainlabels=dv[1:40000,1]
testlabels=dv[40001:53940,1]
class(trainlabels)
trainlabels=factor(trainlabels,levels=c(1,2,3,4,5))
class(trainlabels)
#apply knn
library(class)
test_pred=knn(train,test,trainlabels,k=89)
sum(is.na(trainlabels))
miss=trainlabels[!complete.cases(trainlabels),]
is.na(trainlabels)
View(is.na(trainlabels))
View(trainlabels)
summary(trainlabels)
View(trainlabels)
library(ggplot2)
View(diamonds)
str(diamonds)
table(diamonds$cut)
iv=diamonds[5:10]
View(iv)
dv=diamonds[2]
View(dv)
dv$cut=factor(dv$cut,levels=c("Fair","Good","Very Good","Premium","Ideal"),
labels=c(1,2,3,4,5))
View(dv)
normalise=function(x)
{
return ((x-min(x))/(max(x)-min(x)))
}
iv=as.data.frame(lapply(iv,normalise))
#split train and test set from iv
train=iv[1:40000,]
test=iv[40001:53940,]
class(train)
#split labels from dv
trainlabels=dv[1:40000,1]
View(trainlabels)
summary(trainlabels)
testlabels=dv[40001:53940,1]
View(testlabels)
summary(testlabels)
#apply knn
library(class)
test_pred=knn(train,test,trainlabels,k=89)
View(train)
sum(is.na(train))
View(test)
sum(is.na(test))
#split labels from dv
trainlabels=dv[1:40000,1]
View(trainlabels)
summary(trainlabels)
testlabels=dv[40001:53940,1]
View(testlabels)
View(diamonds)
d1=diamonds[1,5,6,7,8,9,10]
View(d1)
d1=as.data.frame(diamonds[1,5,6,7,8,9,10])
View(d1)
d1=diamonds[,c(1,5,6,7,8,9,10)]
View(d1)
library(ggplot2)
View(diamonds)
str(diamonds)
table(diamonds$cut)
d1=diamonds[,c(2,5,6,7,8,9,10)]
View(d1)
d1$cut=factor(d1$cut,levels=c("Fair","Good","Very Good","Premium","Ideal"),
labels=c(1,2,3,4,5))
View(d1)
normalise=function(x)
{
return ((x-min(x))/(max(x)-min(x)))
}
norml=as.data.frame(lapply(d1[2:7],normalise))
summary(norml)
#split train and test set from iv
train=d1[1:40000,]
View(train)
sum(is.na(train))
test=d1[40001:53940,]
View(test)
sum(is.na(test))
#split train and test set from iv
train=norml[1:40000,]
View(train)
sum(is.na(train))
test=d1[40001:53940,]
View(test)
test=norml[40001:53940,]
View(test)
sum(is.na(test))
#split labels from dv
trainlabels=d1[1:40000,1]
View(trainlabels)
summary(trainlabels)
testlabels=d1[40001:53940,1]
View(testlabels)
summary(testlabels)
#apply knn
library(class)
test_pred=knn(train,test,trainlabels,k=89)
class(train)
class(trainlabels)
class(d1$cut)
#split labels from dv
trainlabels=d1[1:40000,d1$cut]
View(trainlabels)
class(trainlabels)
#split labels from dv
trainlabels=d1$cut[1:40000]
summary(trainlabels)
class(trainlabels)
class(train)
class(trainlabels)
#apply knn
library(class)
test_pred=knn(train,test,trainlabels,k=89)
confusionMatrix(test_pred,testlabels)
library(caret)
confusionMatrix(test_pred,testlabels)
class(test_pred)
class(testlabels)
testlabels=d1$cut[40001:53940]
class(train)
class(testlabels)
#apply knn
library(class)
test_pred=knn(train,test,trainlabels,k=89)
class(test_pred)
library(caret)
confusionMatrix(test_pred,testlabels)
d1=read.csv(file.choose())
View(d1)
table(d1$Outcome)
sum(is.na(d1))
summary(d1)
normalise=function(x)
{
return ((x-min(x))/(max(x)-min(x)))
}
norm_d1=as.data.frame(lapply(d1[1:8]),normalise)
norm_d1=as.data.frame(lapply(d1[1:8],normalise))
View(norm_d1)
summary(norm_d1)
library(caTools)
set.seed(123)
split = sample.split(d1$Outcome, SplitRatio = 0.7)
training_set = subset(d1, split == TRUE)
test_set = subset(d1, split == FALSE)
View(training_set)
View(test_set)
split = sample.split(norm_d1$Outcome, SplitRatio = 0.7)
training_set = subset(norm_d1, split == TRUE)
test_set = subset(norm_d1, split == FALSE)
View(training_set)
View(training_set)
View(test_set)
mtcars
View(mtcars)
d1=mtcars
View(d1)
str(d1)
table(d1$cyl)
table(d1$vs)
table(d1$am)
table(d1$gear)
table(d1$carb)
table(d1$mpg)
View(d1)
iv=d1[c(1,3,4,5,6,7)]
View(iv)
dv=d1["vs"]
View(dv)
class(dv)
dv$vs=factor(dv$vs,levels=c(0,1))
class(dv)
dv$vs=factor(dv$vs,levels=c(0,1))
class(dv$vs)
summary(iv)
return ((x-min(x))/(max(x)-min(x)))
normalise=function(x)
{
return ((x-min(x))/(max(x)-min(x)))
}
norm_iv=as.data.frame(lapply(iv,normalise))
summary(norm_iv)
split = sample.split(norm_iv$mpg, SplitRatio = 0.7)
library(caTools)
set.seed(123)
split = sample.split(norm_iv$mpg, SplitRatio = 0.7)
training_set = subset(norm_iv, split == TRUE)
test_set = subset(norm_iv, split == FALSE)
View(training_set)
View(test_set)
set.seed(123)
split = sample.split(dv$vs, SplitRatio = 0.7)
trainlabels = subset(dv, split == TRUE)
testlabels = subset(dv, split == FALSE)
d1=mtcars
str(d1)
table(d1$cyl)
table(d1$vs)
table(d1$am)
table(d1$gear)
table(d1$carb)
table(d1$mpg)
iv=d1[c(1,3,4,5,6,7)]
dv=d1["vs"]
class(dv)
dv$vs=factor(dv$vs,levels=c(0,1))
class(dv$vs)
summary(iv)
normalise=function(x)
{
return ((x-min(x))/(max(x)-min(x)))
}
norm_iv=as.data.frame(lapply(iv,normalise))
summary(norm_iv)
trained=norm_iv[1:22,]
testset=norm_iv[23:32,]
trainlabels=dv[1:22,1]
testlabels=dv[23:32,1]
library(class)
pred=knn(trained,testset,trainlabels,k=8)
class(trainlabels)
trainlabels=dv$vs[1:22,1]
trainlabels=dv$vs[1:22]
testlabels=dv$vs[23:32]
library(class)
pred=knn(trained,testset,trainlabels,k=8)
library(caret)
confusionMatrix(pred,testlabels)
library(ggplot2)
d1=diamonds
View(d1)
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
setwd("C:/1_Study material/Machine Learning using R/Machine Learning A-Z (Codes and Datasets)/Part 3 - Classification/Section 18 - Naive Bayes/R")
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
View(dataset)
summary(dataset)
dataset=dataset[-1]
table(dataset$Gender)
table(dataset$Purchased)
dataset=dataset[-1]
class(dataset$Purchased)
dataset$Purchased=factor(dataset$Purchased,levels=c(0,1))
class(dataset$Purchased)
dataset[1:2]=scale(dataset[1:2])
summary(dataset)
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
split
pred=naiveBayes(x=training_set,y=dataset$Purchased)
# install.packages('e1071')
library(e1071)
pred=naiveBayes(x=training_set,y=dataset$Purchased)
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# install.packages('e1071')
library(e1071)
pred=naiveBayes(x=training_set,y=dataset$Purchased)
pred=naiveBayes(x=training_set[1:2],y=dataset$Purchased)
View(training_set)
View(test_set)
training_set[1:2] = subset(dataset, split == TRUE)
test_set[1:2] = subset(dataset, split == FALSE)
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
View(dataset)
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
View()
View(training_set)
View(test_set)
# Fitting SVM to the Training set
# install.packages('e1071')
library(e1071)
classifier = naiveBayes(x = training_set[-3],
y = training_set$Purchased)
pred=predict(classifier,test_set[-3])
pred
d1=data.frame("Age"=37,"EstimatedSalary"=67000)
pred1=predict(classifier,d1)
pred1
# Making the Confusion Matrix
library(caret)
cm = table(test_set[, 3], y_pred)
cm
cm = table(test_set[, 3], pred)
cm
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
clear()
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
View(dataset)
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
View(training_set)
# Fitting SVM to the Training set
# install.packages('e1071')
library(e1071)
classifier = naiveBayes(x = training_set[-3],
y = training_set$Purchased)
pred=predict(classifier,test_set[-3])
pred
d1=data.frame("Age"=37,"EstimatedSalary"=67000)
pred1=predict(classifier,d1)
pred1
d1=data.frame("Age"=37,"EstimatedSalary"=87000)
pred1=predict(classifier,d1)
pred1
source("C:/1_Study material/Machine Learning using R/Machine Learning A-Z (Codes and Datasets)/Part 3 - Classification/Section 18 - Naive Bayes/R/naive_bayes.R", echo=TRUE)
d1=data.frame("Age"=42,"EstimatedSalary"=87000)
pred1=predict(classifier,d1)
pred1
d1=data.frame("Age"=47,"EstimatedSalary"=87000)
pred1=predict(classifier,d1)
pred1
View(dataset)
d1=data.frame("Age"=32,"EstimatedSalary"=15000)
pred1=predict(classifier,d1)
pred1
d1=data.frame("Age"=32,"EstimatedSalary"=150000)
pred1=predict(classifier,d1)
pred1
d1=data.frame("Age"=27,"EstimatedSalary"=137000)
pred1=predict(classifier,d1)
pred1
confusionMatrix(test_set[,3],pred)
