labels=c(1,0))
d1
#Encode the dependent variable to make it factor
d1$loan_status=factor(d1$loan_status,
levels=c(" Approved","Rejected"),
labels=c(1,0))
d1
#Encode the dependent variable to make it factor
d1$loan_status=factor(d1$loan_status,
levels=c("Approved","Rejected"),
labels=c(1,0))
d1=read.csv(file.choose())
table(d1$loan_status)
sum(is.na(d1))
head(d1$loan_status)
#Encode the dependent variable to make it factor
d1$loan_status=factor(d1$loan_status,
levels=c(" Approved"," Rejected"),
labels=c(1,0))
sum(is.na(d1))
head(d1$loan_status)
colnames(d1)
d1=d1[c("loan_status","income_annum",
"loan_amount","loan_term","cibil_score")]
sum(is.na(d1))
#Splitting dataset
library(caTools)
split=sample.split(d1$loan_status,SplitRatio = 0.75)
train=subset(d1,split==TRUE)
test=subset(d1,split==FALSE)
1067+3202
View(d1)
#scaling independent variables
train[-1]=scale(train[-1])
test[-1]=scale(test[-1])
#Model Building--
library(class)
classifier1=knn(train=train,test=test,k=9,cl=train$loan_status)
library(e1071)
classifier2=naiveBayes(train[-1],train$loan_status)
library(rpart)
classifier3=rpart(formula=loan_status~.,data=train)
library(randomForest)
#ensemble algorithm-combining multiple algorithms-
#Random forest is combination of multiple decision trees
classifier4=randomForest(x=train[-1],y=train$loan_status,
ntrees=100)
d1=read.csv(file.choose())
View(d1)
table(d1$loan_status)
sum(is.na(d1))
class(d1$loan_status)
head(d1$loan_status)
#Encode the dependent variable to make it factor
d1$loan_status=factor(d1$loan_status,
levels=c(" Approved"," Rejected"),
labels=c(1,0))
#Model Building--knn, naive bayes, decision tree
library(class)
classifier1=knn(train=train,test=test,k=9,cl=train$loan_status)
library(caret)
confusionMatrix(classifier1,test$loan_status)
library(e1071)
classifier2=naiveBayes(x=train[-1],y=train$loan_status)
y2=predict(classifier2,test[-1])
library(caret)
confusionMatrix(classifier2,test$loan_status)
confusionMatrix(y2,test$loan_status)
library(rpart)
classifier3=rpart(formula=loan_status~.,data=train)
library(randomForest)
#ensemble algorithm-combining multiple algorithms-
#Random forest is combination of multiple decision trees
classifier4=randomForest(x=train[-1],y=train$loan_status,
ntrees=100)
library(caret)
y3=predict(classifier4,test[-1])
confusionMatrix(y3,test$loan_status)
d1=read.csv(file.choose())
View(d1)
#checking dependent variable for classes
table(d1$Profit)
#checking missing values
sum(is.na(d1))
#Eliminating unnecessary columns
d1=d1[-4]
colnames(d1)[1]="RD_Spend"
colnames(d1)[3]="MarketingSpend"
#Splitting dataset
library(caTools)
split=sample.split(d1$Profit,SplitRatio = 0.8)
training=subset(d1,split==TRUE)
testing=subset(d1,split==FALSE)
#Implement Multiple Linear Regression
regressor=lm(formula=Profit~.,data=training)
#Implement Multiple Linear Regression
regressor=lm(formula=Profit~.,data=training)
summary(regressor)
library(ModelMetrics)
#Implement Multiple Linear Regression
regressor1=lm(formula=Profit~.,data=training)
summary(regressor1)
#Implement Multiple Linear Regression
regressor1=lm(formula=Profit~.,data=training)
summary(regressor1)
ypred1=predict(regressor1,testing[-4])
library(ModelMetrics)
rmse(ypred1,testing[4])
rmse(ypred1,testing$Profit)
d1$x2=d1$RD_Spend^2
d1$x3=d1$RD_Spend^3
d1$x4=d1$RD_Spend^4
#implementing Polynomial Regression
regressor2=lm(formula=Profit~RD_Spend+x2+x3+x4,
data=training[c(-2,-3)])
d1=d1[c(-2,-3)]
library(caTools)
split=sample.split(d1$Profit,SplitRatio = 0.8)
training2=subset(d1,split==TRUE)
testing2=subset(d1,split==FALSE)
#implementing Polynomial Regression
regressor2=lm(formula=Profit~.,
data=training2)
ypred2=predict(regressor2,testing2[-2])
summary(regressor2)
library(ModelMetrics)
rmse(ypred2,testing2$Profit)
d1=read.csv(file.choose())
View(d1)
colnames(d1)
d1=d1[c("Attrition","MonthlyIncome","MonthlyRate","HourlyRate")]
lapply(d1,summary)
sum(is.na(d1))
#scaling
d1[-1]=scale(d1[-1])
class(d1$Attrition)
d1$Attrition=factor(d1$Attrition,
levels=c("Yes","No"),
labels=c(1,0))
class(d1$Attrition)
#splitting dataset
library(caTools)
t1=sample.split(d1$Attrition,SplitRatio = 0.75)
train1=subset(d1,t1==TRUE)
test1=subset(d1,t1==FALSE)
#Model1-KNN
library(class)
cl1=knn(train1[-1],test1[-1],k=9,cl=train1$Attrition)
cl1=knn(train1,test1[-1],k=9,cl=train1$Attrition)
cl1=knn(train1[-1],test1[-1],k=9,cl=train1$Attrition)
library(caret)
confusionMatrix(cl1,test$Attrition)
confusionMatrix(cl1,test1$Attrition)
cl1=knn(train1[-1],test1[-1],k=12,cl=train1$Attrition)
library(caret)
confusionMatrix(cl1,test1$Attrition)
cl1=knn(train1[-1],test1[-1],k=21,cl=train1$Attrition)
library(caret)
confusionMatrix(cl1,test1$Attrition)
cl1=knn(train1[-1],test1[-1],k=90,cl=train1$Attrition)
library(caret)
confusionMatrix(cl1,test1$Attrition)
cl1=knn(train1[-1],test1[-1],k=22,cl=train1$Attrition)
library(caret)
confusionMatrix(cl1,test1$Attrition)
#Model2--Naive bayes
library(e1071)
cl2=naiveBayes(x=train1[-1],y=train1[1])
y1=predict(cl2,test1[-1])
library(caret)
confusionMatrix(y1,test1$Attrition)
#Model3--decision tree
library(rpart)
cl3=rpart(formula=Attrition~.,data=train1,
method='class')
y2=predict(cl3,test1$Attrition)
y2=predict(cl3,test1[-1])
confusionMatrix(y2,test1$Attrition)
confusionMatrix(y2,test1$Attrition,type='class')
y2=predict(cl3,test1[-1],type='class')
confusionMatrix(y2,test1$Attrition)
cl4=randomForest(formula=Attrition~.,data=train1)
#Model4--Random Forest
library(randomForest)
cl4=randomForest(formula=Attrition~.,data=train1)
y2=predict(cl3,test1[-1],type='class')
y2=predict(cl4,test1[-1],type='class')
y3=predict(cl4,test1[-1],type='class')
confusionMatrix(y3,test1$Attrition)
d1=read.csv(file.choose())
View(d1)
#Eliminating unnecessary columns
d1=d1[c("income_annum","loan_amount","loan_term","cibil_score")]
#renaming columns
colnames(d1)
#renaming columns
colnames(d1)[1]="annual_income"
colnames(d1)
#checking the missing values
sum(is.na(d1))
cor(d1$loan_amount,d1$annual_income)
cor(d1$loan_amount,d1$loan_term)
cor(d1$loan_amount,d1$cibil_score)
#splitting dataset
library(caTools)
t1=sample.split(d1$loan_amount,SplitRatio = 0.75)
train3=subset(d1,t1==TRUE)
test3=susbet(d1,t1==FALSE)
test3=subset(d1,t1==FALSE)
d1=d1[-3]
test3[-2]=scale(test3[-2])
#scaling
train3[-2]=scale(train3[-2])
#Multiple Linear regression
r1=lm(formula=loan_amount~.,data=train3)
y1=predict(r1,test[-2])
y1=predict(r1,test3[-2])
library(ModelMetrics)
rmse(y1,test3$loan_amount)
d2=data.frame("annual_income"=960000,"cibil_score"=875)
y1=predict(r1,d2)
y2=predict(r1,d2)
View(d1)
d1=read.csv(file.choose())
#Eliminating unnecessary columns
d1=d1[c("income_annum","loan_amount","loan_term","cibil_score")]
#renaming columns
colnames(d1)[1]="annual_income"
#checking the missing values
sum(is.na(d1))
cor(d1$loan_amount,d1$annual_income)
cor(d1$loan_amount,d1$loan_term)
cor(d1$loan_amount,d1$cibil_score)
d1=d1[-3]
#splitting dataset
library(caTools)
t1=sample.split(d1$loan_amount,SplitRatio = 0.75)
train3=subset(d1,t1==TRUE)
test3=subset(d1,t1==FALSE)
#scaling
train3[-2]=scale(train3[-2])
test3[-2]=scale(test3[-2])
#Multiple Linear regression
r1=lm(formula=loan_amount~.,data=train3)
y1=predict(r1,test3[-2])
library(ModelMetrics)
rmse(y1,test3$loan_amount)
d2=data.frame("annual_income"=960000,
"cibil_score"=875)
y2=predict(r1,d2)
y2
#polynomial regression
d1$x2=d1$annual_income^2
d1$x3=d1$annual_income^3
cor(d1$loan_amount,d1$x2)
cor(d1$loan_amount,d1$x3)
#scaling again
d1[-2]=scale(d1[-2])
#splitting dataset again
library(caTools)
t2=sample.split(d1$loan_amount,SplitRatio = 0.75)
train4=subset(d1,t2==T)
test4=subset(d1,t2==F)
View(test4)
r2=lm(formula=loan_amount~annual_income+x2+x3,data=train3)
r2=lm(formula=loan_amount~annual_income+x2+x3,data=train4)
y2=predict(r1,test4[c(-2,-3)])
y2=predict(r2,test4[c(-2,-3)])
library(ModelMetrics)
rmse(y2,test4$loan_amount)
d2=data.frame("annual_income"=960000,
"x2"=960000^2,"x3"=960000^3)
y3=predict(r2,d2)
y2
y3
d1=read.csv(file.choose())
View(d1)
d1=d1[-1]
sum(is.na(d1))
class(d1$diagnosis_result)
d1$diagnosis_result=factor(d1$diagnosis_result,
levels=c("B","M"),
labels=c(1,0))
class(d1$diagnosis_result)
#scaling
d1[-1]=scale(d1[-1])
#splitting dataset
library(caTools)
s1=sample.split(d1$diagnosis_result,SplitRatio = 0.75)
t1=subset(d1,s1==T)
t2=subset(d1,s1==F)
#model1-knn
library(class)
classifier1=knn(t1,t2,k=7,cl=t1$diagnosis_result)
#model2--naive bayes
library(e1071)
classifier2=naiveBayes(x=t1[-1],y=t1$diagnosis_result)
y2=predict(classifier2,t2[-1])
library(caret)
confusionMatrix(t2$diagnosis_result,y2)
confusionMatrix(classifier1,t2$diagnosis_result)
#model3-Decision Tree
library(rpart)
classifier3=rpart(formula=diagnosis_result,
data=t1)
classifier3=rpart(formula=diagnosis_result~.,
data=t1)
y3=predict(classifier3,t2[-1])
library(caret)
confusionMatrix(y3,t1$diagnosis_result)
confusionMatrix(y3,t2$diagnosis_result)
confusionMatrix(t2$diagnosis_result,y3)
y3
y3=predict(classifier3,t2[-1],type'class')
y3=predict(classifier3,t2[-1],type='class')
library(caret)
confusionMatrix(t2$diagnosis_result,y3)
d1=read.csv(file.choose())
View(d1)
table(d1$depression_severity)
sum(is.na(d1))
missing=d1[!complete.cases(d1),]
missing
missing=d1[!complete.cases(d1),8]
missing
sum(is.na(d1$depression_severity))
d1$depression_severity=na.omit(d1$depression_severity)
na.omit(d1$depression_severity)
d1$depression_severity=na.omit(d1$depression_severity)
d1=read.csv(file.choose())
View(d1)
d1=read.csv(file.choose())
table(d1$depression_severity)
sum(is.na(d1))
sum(is.na(d1$depression_severity))
d1=d1[complete.cases(d1[1])]
d1=d1[complete.cases(d1[1]),]
View(d1)
sum(is.na(d1$depression_severity))
lapply(d1,summary)
d1=read.csv(file.choose())
View(d1)
lapply(d1,summary)
d1=d1[c("age","bmi","phq_score","gad_score","epworth_score","depression_severity")]
d1=d1[complete.cases(d1[6]),]
sum(is.na(d1$depression_severity))
#Load the dataset
d1=read.csv(file.choose())
View(d1)
#Choose the dependent and independent variables
#depression_severity--dependent variable
d1=d1[c("age","bmi","phq_score","gad_score",
"epworth_score","depression_severity")]
lapply(d1,summary)
table(d1$depression_severity)
sum(is.na(d1$depression_severity))
d1=d1[complete.cases(d1[6],)]
d1=d1[complete.cases(d1[6]),]
sum(is.na(d1$depression_severity))
lapply(d1,summary)
d1$epworth_score=ifelse(is.na(d1$epworth_score),
mean(d1$epworth_score,na.rm=T),
d1$epworth_score)
table(d1$depression_severity)
lapply(d1,summary)
class(d1$depression_severity)
table(d1$depression_severity)
head(d1$depression_severity)
table(d1$depression_severity)
d1$depression_severity=factor(d1$depression_severity,
levels=c("none","None-minimal","Mild",
"Moderate","Moderately severe",
"Severe"),
labels=c(0,1,2,3,4,5))
class(d1$depression_severity)
#no scaling bcz values range between 0 to 52 in independent columns
#splitting dataset
library(caTools)
split=sample.split(d1$depression_severity,SplitRatio = 0.75)
train=subset(d1,split==T)
test=subset(d1,split==F)
#Model1--knn
library(class)
classifier1=knn(train,test,cl=train$depression_severity,k=9)
library(caret)
confusionMatrix(classifier1,test$depression_severity)
#accuracy==90.21%
#Model2--Naive Bayes
library(e1071)
classifier2=naiveBayes(x=train[-6],y=train$depression_severity)
y2=predict(classifier2,test[-6])
library(caret)
confusionMatrix(classifier2,test$depression_severity)
confusionMatrix(y2,test$depression_severity)
#accuracy=96.39
#model3--decision tree
library(rpart)
classifier3=rpart(formula=depression_severity~.,data=train)
y3=predict(classifier3,test[-6])
confusionMatrix(y3,test$depression_severity)
confusionMatrix(y3,test$depression_severity,type='class')
y3=predict(classifier3,test[-6],type='class')
confusionMatrix(y3,test$depression_severity)
#Accuracy=100%
d2=data.frame("age"=18,"bmi"=22.10029,"phq_score"=7,"gad_score"=12,"epworth_score"=9)
y0=predict(classifier3,d2,type='class')
y0
#model4--RandomForest
library(randomForest)
classifier4=randomForest(formula=depression_severity~.,data=train,ntrees=50)
y3=predict(classifier3,test[-6],type='class')
confusionMatrix(y3,test$depression_severity)
setwd("C:/1_Study material/Machine Learning using R/Machine Learning A-Z (Codes and Datasets)/Part 4 - Clustering/Section 24 - K-Means Clustering/R")
# K-Means Clustering
# Importing the dataset
dataset = read.csv('Mall_Customers.csv')
View(dataset)
dataset = dataset[4:5]
View(dataset)
# Using the elbow method to find the optimal number of clusters
set.seed(6)
wcss = vector()
# visualization
kmeans1= kmeans(x=dataset,centers=4,nstart = 10)
kmeans2= kmeans(x=dataset,centers=6,nstart = 12)
kmeans1
kmeans1$withinss
kmeans1$iter
kmeans$cluster
kmeans1$cluster
wcss = vector()
for (i in 1:10)
{
wcss[i] = sum(kmeans(dataset, i)$withinss)
}
plot(1:10,
wcss,
type = 'b',
main = paste('The Elbow Method'),
xlab = 'Number of clusters',
ylab = 'WCSS')
#install.packages('ggfortify')
library(ggfortify)
plot1=autoplot(kmeans1,dataset,frame= TRUE)
plot2=autoplot(kmeans2,dataset,frame= TRUE)
library(gridExtra)
grid.arrange(plot1,plot2,nrow=1)
setwd("C:/1_Study material/Machine Learning using R/Programs/Unit 5")
#install.packages("arules")
library(arules)
getwd()
groceries <- read.transactions("groceries.csv", sep = ",")
#it results in a sparse matrix suitable for transactional data.
summary(groceries)
#details about no of transactions, no of items, density of non zero,
inspect(groceries[1:5])
#provide the detail of first 5 transactions
itemFrequency(groceries[, 1:3])
#allows us to see the proportion of transactions that contain the
#item and to view the support level for
#the first three items in the grocery data
itemFrequencyPlot(groceries, support = 0.1)
#plot the bar chart using atleat 10% of support
itemFrequencyPlot(groceries, topN = 10)
#plot with 20 items
image(groceries[1:5])
#combining it with the sample() function, you can view the sparse matrix
#for a randomly sampled set of
#transactions.
apriori(groceries)
#by default support is 0.1 and confidence is 0.8
groceryrules <- apriori(groceries, parameter =
list(support =
0.006, confidence = 0.25, minlen = 2))
write(groceryrules, file = "groceryrules.csv",
sep = ",", quote = TRUE, row.names = FALSE)
groceryrules
#write rules to the csv file
groceryrules_df <- as(groceryrules, "data.frame")
str(groceryrules_df)
# K-Means Clustering
# Importing the dataset
dataset = read.csv('Mall_Customers.csv')
setwd("C:/1_Study material/Machine Learning using R/Machine Learning A-Z (Codes and Datasets)/Part 4 - Clustering/Section 24 - K-Means Clustering/R")
# K-Means Clustering
# Importing the dataset
dataset = read.csv('Mall_Customers.csv')
View(dataset)
dataset = dataset[4:5]
View(dataset)
# visualization
kmeans1= kmeans(x=dataset,centers=4,nstart = 10)
kmeans1
kmeans1$cluster
kmeans1$withinss
#install.packages('ggfortify')
library(ggfortify)
plot1=autoplot(kmeans1,dataset,frame= TRUE)
plot2=autoplot(kmeans2,dataset,frame= TRUE)
library(gridExtra)
grid.arrange(plot1,plot2,nrow=1)
wcss = vector()
for (i in 1:10)
{
wcss[i] = sum(kmeans(dataset, i)$withinss)
}
plot(1:10,
wcss,
type = 'b',
main = paste('The Elbow Method'),
xlab = 'Number of clusters',
ylab = 'WCSS')
